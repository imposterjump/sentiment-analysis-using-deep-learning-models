{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available? True\n",
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\akram\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\akram\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\akram\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Imports, NLTK setup & GPU check\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import nltk\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# NLTK downloads (only the first time)\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# GPU check\n",
    "print(\"CUDA available?\", torch.cuda.is_available())\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (20000, 4) Val: (5000, 4) Test: (10000, 3)\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "text",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "text_len",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "score",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "label",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "37346790-4565-499a-b587-509a07cedc84",
       "rows": [
        [
         "7522",
         "Many reviews I've read reveals that most people tend to like Part One better than Part Two. I feel exactly the opposite. Part One played around a bit much with trying to find different ways of showing Che Guevara's personality through different types of film stock, different locations, and cutting back and forth between an interview and the Cuban revolution. For the most part it was structured finely but somewhat distracting. In Part Two, Che enters Bolivia, and along with changing geographical location, the rules and the structure changes. Gone are the spacial jumps and switching between stocks, the \"documentary realism\" and the treatises. Instead, now we are literally trapped with Che in a desaturated, depopulated landscape where the only people who exist are burdened too far with their lives for anything but survival to be an option. I posit that it's the dark turn of Che's life that is the real reason why most people prefer Part One to Part Two.The change in geographic location also signifies, for me at least, that Che: Part One and Che: Part Two are, in fact, the second two acts of a three act structure begun by Motorcycle Diaries. Motorcycle Diaries is Che's coming-of-age (or more appropriately, coming-of-ideals) in Argentina, Che: Part One is his military leadership in Cuba, and Che: Part Two is his downfall in Bolivia. These movies do not completely illustrate his life (we're missing his experiences in Guatamala and, more importantly in my opinion, his post-Cuban revolution executions), but they create a very detailed exploration into the controversial aspects of his character and nature as worldwide symbol. He both symbolizes the idealism and need for armed resistance to oppression, and revolutionary failings in the post-World War II third world countries and their hindering by such activities as, um, the CIAs meddling.But, yet again, all of that is projected on-screen in this case not through long scenes of dialog and speeches, but through a much more intimate, suffering portrayal of Che at the end of his thread and his life. Again, the rules have changed, and in this case it's hard to tell if there was any chance of success at all. The number of times the camera shows people literally trapped between a rock and a hard place and the desaturated, shaky long takes involves the audience into the narrative of people imprisoned in a hostile landscape, an existential hell, where revolutionary beliefs ultimately end up taking second tier to the desperation of hunted people starving to death. It's just not that easy of a movie to watch, but it's very effective.--PolarisDiB",
         "2621",
         "9",
         "1"
        ],
        [
         "17493",
         "The effect achieved in this story about a psychiatrist who becomes involved with con artists is so mannered that I have to assume that that was the desired intent. The sets are artificial and at no time did I not feel that I was watching a movie. It seemed like the actors were just reading their lines, rather than responding to one another. While the film has elements of early film noir (except that it is in color) the approach is so exaggerated that I almost have to conclude that it is a parody of the genre.Given that the presentation had no appeal to me, I was at least expecting an engaging story. Usually I am pretty slow on the uptake when it comes to stories with plot twists, but you could see what was coming here within the first fifteen minutes. By the time of the, \"Gee, I forgot the $80,000,\" moment, I thought to myself that this thing is truly ridiculous. For a psychiatrist with stated experience in gambling addictions to behave so stupidly is beyond belief. If at any stage she had behaved like a normal intelligent person, the whole story would have fallen apart.This wooden production left me cold.",
         "1123",
         "3",
         "0"
        ],
        [
         "12260",
         "I really enjoyed the performances of the main cast. Emma Lung is courageous and interesting. The director has developed performances where the characters are not one dimensional. A complex story with the changing between eras. Also appreciated the underlying story of the unions losing power and the effect of a large employer closing on a small town. I do not agree with the comment that the older man has to be attractive. There have be many relationships with older men and younger women - without the male being good looking. Depth of character can be appealing to the not so shallow. The film has a good look and the cinematography is also good.",
         "650",
         "8",
         "1"
        ],
        [
         "1275",
         "'Midnight Cowboy' was rated X with the original release back in 1969. There are some scenes where you can understand that, just a little. The movie about Joe Buck (Jon Voight) coming from Texas to New York City to become a hustler is sometimes a little disturbing. Dressed up as a cowboy he tries to live as a hustler, making money by the act of love. It does not work out as he planned. After a guy named Rico 'Ratso' Rizzo (Dustin Hoffman) first pulled a trick on him and stole some money they become friends. They live in an empty and very filthy apartment. Then Ratso gets sick and Joe has to try to make some money.The movie was probably rated X for the main subject but on the way we see some strange things. The editing in this movie is great. We see dream sequences from Joe and Ratso interrupted by the real world in a nice and sometimes funny way. Dustin Hoffman, Jon Voight and the supporting actors give great performances. Especially Hoffman delivers some fine famous lines. The score is done by John Barry and sounds great. All this makes this a great movie that won the Best Picture Oscar for a good reason.",
         "1122",
         "9",
         "1"
        ],
        [
         "8004",
         "As the maker of \"This Darkness,\" I admit we neglected 3 very important acknowledgments in our end credits. The omissions were over-sights that could not be corrected once committed, nor did the parties involved --- who saw the movie --- mention it at the time. On behalf of the excellent cast and crew of the film, I extend them an apology. Obviously, some criticisms posted here are harsh in light of their credit being accidentally. Our production values were negligible and our \"special effects\" were quite special indeed, but the plot is very strong and the cinematography by John McLeod is superb. We hope you, the reader, enjoy \"This Darkness\" and the efforts of those who worked their butts off for free. Thank you, Dylan O'Leary, Director.",
         "747",
         "10",
         "1"
        ]
       ],
       "shape": {
        "columns": 4,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_len</th>\n",
       "      <th>score</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7522</th>\n",
       "      <td>Many reviews I've read reveals that most peopl...</td>\n",
       "      <td>2621</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17493</th>\n",
       "      <td>The effect achieved in this story about a psyc...</td>\n",
       "      <td>1123</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12260</th>\n",
       "      <td>I really enjoyed the performances of the main ...</td>\n",
       "      <td>650</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1275</th>\n",
       "      <td>'Midnight Cowboy' was rated X with the origina...</td>\n",
       "      <td>1122</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8004</th>\n",
       "      <td>As the maker of \"This Darkness,\" I admit we ne...</td>\n",
       "      <td>747</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  text_len  score  \\\n",
       "7522   Many reviews I've read reveals that most peopl...      2621      9   \n",
       "17493  The effect achieved in this story about a psyc...      1123      3   \n",
       "12260  I really enjoyed the performances of the main ...       650      8   \n",
       "1275   'Midnight Cowboy' was rated X with the origina...      1122      9   \n",
       "8004   As the maker of \"This Darkness,\" I admit we ne...       747     10   \n",
       "\n",
       "       label  \n",
       "7522       1  \n",
       "17493      0  \n",
       "12260      1  \n",
       "1275       1  \n",
       "8004       1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 2: Load CSVs & train/val/test split\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Adjust paths if needed\n",
    "train_df = pd.read_csv(r'C:\\NLP project\\Sentiment-Analysis-using-LSTM\\train.csv')\n",
    "test_df  = pd.read_csv(r'C:\\NLP project\\Sentiment-Analysis-using-LSTM\\test.csv').iloc[:10000].reset_index(drop=True)\n",
    "\n",
    "# Split 25k train → 20k train + 5k val\n",
    "train_df, val_df = train_test_split(\n",
    "    train_df,\n",
    "    test_size=5000,\n",
    "    random_state=42,\n",
    "    stratify=train_df['label']\n",
    ")\n",
    "\n",
    "print(\"Train:\", train_df.shape, \"Val:\", val_df.shape, \"Test:\", test_df.shape)\n",
    "train_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample cleaned review:\n",
      "many review read reveals people tend like part one better part two feel exactly opposite part one played around bit much trying find different way showing che guevara personality different type film stock different location cutting back forth interview cuban revolution part structured finely somewhat distracting part two che enters bolivia along changing geographical location rule structure change gone spacial jump switching stock documentary realism treatise instead literally trapped che desaturated depopulated landscape people exist burdened far life anything survival option posit dark turn che life real reason people prefer part one part two change geographic location also signifies least che part one che part two fact second two act three act structure begun motorcycle diary motorcycle diary che coming age appropriately coming ideal argentina che part one military leadership cuba che part two downfall bolivia movie completely illustrate life missing experience guatamala importantly opinion post cuban revolution execution create detailed exploration controversial aspect character nature worldwide symbol symbolizes idealism need armed resistance oppression revolutionary failing post world war ii third world country hindering activity um cia meddling yet projected screen case long scene dialog speech much intimate suffering portrayal che end thread life rule changed case hard tell chance success number time camera show people literally trapped rock hard place desaturated shaky long take involves audience narrative people imprisoned hostile landscape existential hell revolutionary belief ultimately end taking second tier desperation hunted people starving death easy movie watch effective polarisdib\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Text cleaning + token preprocessing\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_text(text):\n",
    "    # 1. Remove HTML\n",
    "    text = BeautifulSoup(text, 'html.parser').get_text()\n",
    "    # 2. Lowercase\n",
    "    text = text.lower()\n",
    "    # 3. Remove non-alphanumeric\n",
    "    text = re.sub(r'[^a-z0-9\\s]', ' ', text)\n",
    "    # 4. Tokenize, remove stopwords, lemmatize\n",
    "    tokens = []\n",
    "    for w in text.split():\n",
    "        if w in stop_words:\n",
    "            continue\n",
    "        tokens.append( lemmatizer.lemmatize(w) )\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# Apply\n",
    "for df in (train_df, val_df, test_df):\n",
    "    df['clean_text'] = df['text'].map(clean_text)\n",
    "\n",
    "print(\"Sample cleaned review:\")\n",
    "print(train_df['clean_text'].iloc[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\sentiment2\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batches → train: 625, val: 157, test: 313\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Tokenization & DataLoaders (using Hugging Face tokenizer)\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "model_name = \"roberta-base\"  # for tokenizer only\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        encoding = self.tokenizer(\n",
    "            self.texts[idx],\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_len,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        item = {k: v.squeeze(0) for k, v in encoding.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "# Datasets\n",
    "train_ds = TextDataset(train_df['clean_text'].tolist(), train_df['label'].tolist(), tokenizer)\n",
    "val_ds   = TextDataset(val_df['clean_text'].tolist(),   val_df['label'].tolist(),   tokenizer)\n",
    "test_ds  = TextDataset(test_df['clean_text'].tolist(),  test_df['label'].tolist(),  tokenizer)\n",
    "\n",
    "# DataLoaders\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"Batches → train: {len(train_loader)}, val: {len(val_loader)}, test: {len(test_loader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNNLSTMSentiment(\n",
      "  (embedding): Embedding(50265, 128, padding_idx=1)\n",
      "  (conv1): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "  (relu): ReLU()\n",
      "  (pool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (lstm): LSTM(128, 128, batch_first=True, bidirectional=True)\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      "  (fc): Linear(in_features=256, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Define a CNN + BiLSTM model from scratch\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class CNNLSTMSentiment(nn.Module):\n",
    "    def __init__(self,\n",
    "                 vocab_size,\n",
    "                 embed_dim=128,\n",
    "                 cnn_filters=128,\n",
    "                 lstm_hidden=128,\n",
    "                 lstm_layers=1,\n",
    "                 kernel_size=3,\n",
    "                 dropout=0.3):\n",
    "        super().__init__()\n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=tokenizer.pad_token_id)\n",
    "        # 1D convolution over the embedding dimension\n",
    "        self.conv1 = nn.Conv1d(\n",
    "            in_channels=embed_dim,\n",
    "            out_channels=cnn_filters,\n",
    "            kernel_size=kernel_size,\n",
    "            padding=kernel_size//2\n",
    "        )\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2)\n",
    "        # Bi-directional LSTM on top of conv features\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=cnn_filters,\n",
    "            hidden_size=lstm_hidden,\n",
    "            num_layers=lstm_layers,\n",
    "            bidirectional=True,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if lstm_layers > 1 else 0\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # Final classification layer\n",
    "        self.fc = nn.Linear(lstm_hidden * 2, 2)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        # (batch, seq_len) → (batch, seq_len, embed_dim)\n",
    "        x = self.embedding(input_ids)\n",
    "        x = self.dropout(x)\n",
    "        # Prepare for conv: (batch, embed_dim, seq_len)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)                  # (batch, cnn_filters, seq_len/2)\n",
    "        # Prepare for LSTM: (batch, seq_len/2, cnn_filters)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        # LSTM: we only care about the hidden state\n",
    "        _, (h_n, _) = self.lstm(x)\n",
    "        # Concatenate forward & backward final hidden states\n",
    "        h = torch.cat((h_n[-2], h_n[-1]), dim=1)\n",
    "        h = self.dropout(h)\n",
    "        return self.fc(h)\n",
    "\n",
    "# Instantiate model\n",
    "vocab_size = tokenizer.vocab_size\n",
    "model = CNNLSTMSentiment(vocab_size).to(device)\n",
    "print(model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 625/625 [00:14<00:00, 44.04it/s, loss=0.426]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Epoch 1 Val Acc: 0.7850\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 625/625 [00:13<00:00, 47.43it/s, loss=0.373]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Epoch 2 Val Acc: 0.8294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 625/625 [00:13<00:00, 47.70it/s, loss=0.287] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Epoch 3 Val Acc: 0.8378\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 625/625 [00:12<00:00, 48.32it/s, loss=0.245] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Epoch 4 Val Acc: 0.8450\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 625/625 [00:13<00:00, 47.67it/s, loss=0.191] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Epoch 5 Val Acc: 0.8464\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 625/625 [00:12<00:00, 48.11it/s, loss=0.236] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Epoch 6 Val Acc: 0.8450\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 625/625 [00:13<00:00, 47.67it/s, loss=0.204] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Epoch 7 Val Acc: 0.8490\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 625/625 [00:12<00:00, 48.16it/s, loss=0.151] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Epoch 8 Val Acc: 0.8478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 625/625 [00:13<00:00, 46.34it/s, loss=0.114] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Epoch 9 Val Acc: 0.8530\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 625/625 [00:13<00:00, 46.92it/s, loss=0.233]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Epoch 10 Val Acc: 0.8468\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Training loop with early stopping\n",
    "\n",
    "from torch.optim import Adam\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "max_epochs = 10\n",
    "patience = 2\n",
    "best_val_acc = 0\n",
    "epochs_no_improve = 0\n",
    "\n",
    "def evaluate(loader):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for b in loader:\n",
    "            inputs = b['input_ids'].to(device)\n",
    "            labels = b['labels'].to(device)\n",
    "            logits = model(inputs)\n",
    "            preds = logits.argmax(dim=1)\n",
    "            all_preds += preds.cpu().tolist()\n",
    "            all_labels += labels.cpu().tolist()\n",
    "    return accuracy_score(all_labels, all_preds)\n",
    "\n",
    "for epoch in range(1, max_epochs+1):\n",
    "    model.train()\n",
    "    loop = tqdm(train_loader, desc=f\"Epoch {epoch}\")\n",
    "    for batch in loop:\n",
    "        inputs = batch['input_ids'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(inputs)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "    val_acc = evaluate(val_loader)\n",
    "    print(f\"→ Epoch {epoch} Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        epochs_no_improve = 0\n",
    "        torch.save(model.state_dict(), 'best_lstm.pth')\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f\"Early stopping after {epoch} epochs\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔥 Best LSTM Test Accuracy: 0.8479\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Load best model and evaluate on test set\n",
    "\n",
    "model.load_state_dict(torch.load('best_lstm.pth'))\n",
    "test_acc = evaluate(test_loader)\n",
    "print(f\"🔥 Best LSTM Test Accuracy: {test_acc:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sentiment2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
